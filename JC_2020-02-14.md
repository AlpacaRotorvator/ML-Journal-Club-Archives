# ML Journal Club
9th Edition ⁠— 2020-02-14

---

## General Discussion

### [Hidden Technical Debt in Machine Learning Systems](https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf)
**Presented by**: Brian K.

**Authors**: Sculley, D. and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael and Crespo, Jean-Francois and Dennison, Dan

**DOI**: N/A

**Abstract**: Machine learning offers a fantastically powerful toolkit for building useful complex prediction systems quickly. This paper argues it is dangerous to think of these quick wins as coming for free. Using the software engineering framework of technical debt, we find it is common to incur massive ongoing maintenance costs in real-world ML systems. We explore several ML-specific risk factors to account for in system design. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, configuration issues, changes in the external world, and a variety of system-level anti-patterns.

### [Notes on Deep Learning for NLP](https://arxiv.org/abs/1808.09772)
**Presented by**: Daniel Vargas

**Authors**: Tixier, Antoine J.-P.

**DOI**: N/A

**Abstract**: My notes on Deep Learning for NLP. 

---

## Featured Paper
**Speaker**: Daniel Vargas

### [Relational Autoencoder for Feature Extraction](https://arxiv.org/abs/1802.03145)
**Authors**: Qinxue Meng, Daniel Catchpoole, David Skillicorn, Paul J. Kennedy

**DOI**: N/A

**Abstract**: Feature extraction becomes increasingly important as data grows high dimensional. Autoencoder as a neural network based feature extraction method achieves great success in generating abstract features of high dimensional data. However, it fails to consider the relationships of data samples which may affect experimental results of using original and new features. In this paper, we propose a Relation Autoencoder model considering both data features and their relationships. We also extend it to work with other major autoencoder models including Sparse Autoencoder, Denoising Autoencoder and Variational Autoencoder. The proposed relational autoencoder models are evaluated on a set of benchmark datasets and the experimental results show that considering data relationships can generate more robust features which achieve lower construction loss and then lower error rate in further classification compared to the other variants of autoencoders. 